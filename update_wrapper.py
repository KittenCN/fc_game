from pathlib import Path
path = Path("fc_emulator/wrappers.py")
text = path.read_text(encoding="utf-8")
old = "class EpsilonRandomActionWrapper(gym.ActionWrapper):\n    \"\"\"Injects epsilon-greedy exploration for discrete action spaces.\"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        epsilon: float,\n        *,\n        skill_actions: tuple[int, ...] | None = None,\n        skill_bias: float = 0.7,\n    ) -> None:\n        super().__init__(env)\n        if not isinstance(env.action_space, gym.spaces.Discrete):\n            raise ValueError(\"EpsilonRandomActionWrapper requires a discrete action space\")\n        self.epsilon = max(0.0, float(epsilon))\n        self._skill_actions: tuple[int, ...] = tuple(skill_actions or ())\n        self._skill_bias = float(min(max(skill_bias, 0.0), 1.0))\n\n    def action(self, action: int) -> int:  # type: ignore[override]\n        if self.epsilon <= 0.0:\n            return action\n        if self.np_random.random() >= self.epsilon:\n            return action\n        if self._skill_actions and self.np_random.random() < self._skill_bias:\n            return int(self.np_random.choice(self._skill_actions))\n        return int(self.action_space.sample())\n\n    def set_exploration_epsilon(self, epsilon: float) -> None:\n        self.epsilon = max(0.0, float(epsilon))\n\n    def set_skill_actions(self, actions: tuple[int, ...], *, bias: float | None = None) -> None:\n        self._skill_actions = tuple(actions)\n        if bias is not None:\n            self._skill_bias = float(min(max(bias, 0.0), 1.0))\n"
new = "class EpsilonRandomActionWrapper(gym.ActionWrapper):\n    \"\"\"Injects epsilon-greedy exploration for discrete action spaces with skill biasing.\"\"\"\n\n    def __init__(\n        self,\n        env: gym.Env,\n        epsilon: float,\n        *,\n        skill_actions: tuple[int, ...] | None = None,\n        skill_sequences: tuple[tuple[int, ...], ...] | None = None,\n        skill_bias: float = 0.7,\n        stagnation_boost: float = 0.3,\n        stagnation_threshold: int = 120,\n    ) -> None:\n        super().__init__(env)\n        if not isinstance(env.action_space, gym.spaces.Discrete):\n            raise ValueError(\"EpsilonRandomActionWrapper requires a discrete action space\")\n        self.epsilon = max(0.0, float(epsilon))\n        self._skill_actions: tuple[int, ...] = tuple(skill_actions or ())\n        self._skill_sequences: list[tuple[int, ...]] = [tuple(seq) for seq in (skill_sequences or ()) if seq]\n        self._skill_bias = float(min(max(skill_bias, 0.0), 1.0))\n        self._stagnation_boost = max(0.0, float(stagnation_boost))\n        self._stagnation_threshold = max(1, int(stagnation_threshold))\n        self._macro_queue: list[int] = []\n\n    def _current_stagnation(self) -> int:\n        unwrapped = getattr(self.env, \"unwrapped\", self.env)\n        return int(getattr(unwrapped, \"stagnation_counter\", 0))\n\n    def action(self, action: int) -> int:  # type: ignore[override]\n        if self._macro_queue:\n            return self._macro_queue.pop(0)\n\n        stagnation = self._current_stagnation()\n        effective_epsilon = self.epsilon\n        if self.epsilon > 0 and stagnation >= self._stagnation_threshold and self._stagnation_boost > 0:\n            ratio = stagnation / float(self._stagnation_threshold)\n            effective_epsilon = min(1.0, self.epsilon + ratio * self._stagnation_boost)\n\n        if effective_epsilon <= 0.0 or self.np_random.random() >= effective_epsilon:\n            return action\n\n        if self._skill_sequences and stagnation >= self._stagnation_threshold and self.np_random.random() < self._skill_bias:\n            seq = tuple(self.np_random.choice(self._skill_sequences))\n            if seq:\n                self._macro_queue.extend(seq[1:])\n                return int(seq[0])\n\n        if self._skill_actions and self.np_random.random() < self._skill_bias:\n            return int(self.np_random.choice(self._skill_actions))\n\n        sampled = int(self.action_space.sample())\n        return sampled\n\n    def set_exploration_epsilon(self, epsilon: float) -> None:\n        self.epsilon = max(0.0, float(epsilon))\n\n    def set_skill_actions(self, actions: tuple[int, ...], *, bias: float | None = None) -> None:\n        self._skill_actions = tuple(actions)\n        if bias is not None:\n            self._skill_bias = float(min(max(bias, 0.0), 1.0))\n\n    def set_skill_sequences(self, sequences: tuple[tuple[int, ...], ...], *, bias: float | None = None) -> None:\n        self._skill_sequences = [tuple(seq) for seq in sequences if seq]\n        if bias is not None:\n            self._skill_bias = float(min(max(bias, 0.0), 1.0))\n"
if old not in text:
    raise SystemExit("old wrapper definition mismatch")
text = text.replace(old, new)
path.write_text(text, encoding="utf-8")